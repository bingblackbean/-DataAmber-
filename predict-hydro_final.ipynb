{"cells":[{"metadata":{"_uuid":"d74d4851-5db5-403a-8acd-a8d438c5163b","_cell_guid":"3b202b19-a3bb-48ad-85b6-5140c84259a5","trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom fbprophet import Prophet, make_holidays\nimport pandas as pd\nfrom keras.layers import Dense, LSTM, Dropout\nfrom keras.models import Sequential\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24e9a356-fa1d-4d4a-93a9-9fe227c7e3ca","_cell_guid":"f71ab884-efc7-4d0d-ad32-170e59e220f9","trusted":true},"cell_type":"code","source":"# 数据路径\ndata_file = '../input/hydrowater/data'\n# read data\nQidata = data_file + '/Qi.xlsx'\nRaindata = data_file + '/Rain_forecast.xlsx'\nEnvironmentdata = data_file + '/Env.xlsx'\nRemoteRaindata = data_file + '/Remote_rain.xlsx'\n\n\n# 数据路径\ndata_file_final = '../input/hydrowater'\n# read data\nQidata_final = data_file_final + '/Qi_final.xlsx'\nRaindata_final = data_file_final + '/Rain_forecast_final.xlsx'\nEnvironmentdata_final = data_file_final + '/Env_final.xlsx'\nRemoteRaindata_final = data_file_final + '/Remote_rain_final.xlsx'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"859912fb-f9d7-4611-9b2d-96786407e337","_cell_guid":"6bf1b966-edd0-4a9f-9995-ceaa7fa39aea","trusted":true},"cell_type":"code","source":"# 读入数据，用TimeStample 作为index，并且转换为时间戳格式\nqi_raw_df = pd.read_excel(Qidata, index_col='TimeStample', parse_dates=True)\nrain_raw_df = pd.read_excel(\n    Raindata,\n    index_col='TimeStample',\n    parse_dates=True)\nenv_raw_df = pd.read_excel(\n    Environmentdata,\n    index_col='TimeStample',\n    parse_dates=True)\nremote_rain_df = pd.read_excel(\n    RemoteRaindata,\n    index_col='TimeStample',\n    parse_dates=True)\n\nqi_raw_df_final = pd.read_excel(Qidata_final, index_col='TimeStample', parse_dates=True)\nrain_raw_df_final = pd.read_excel(\n    Raindata_final,\n    index_col='TimeStample',\n    parse_dates=True)\nenv_raw_df_final = pd.read_excel(\n    Environmentdata_final,\n    index_col='TimeStample',\n    parse_dates=True)\nremote_rain_df_final = pd.read_excel(\n    RemoteRaindata_final,\n    index_col='TimeStample',\n    parse_dates=True)\nrain_raw_df_final.drop('5天合计',inplace=True,axis=1)\n\n\n\n# resample and fillna 重采样，fillna\nrain_raw_df.loc[pd.Timestamp('2018-11-08')\n] = rain_raw_df.loc[pd.Timestamp('2018-11-07')]  # 重采样会放弃2018-11-07的样本，所以在此加上2018-11-08\nenv_raw_df.loc[pd.Timestamp('2018-11-01')\n] = env_raw_df.loc[pd.Timestamp('2018-10-31')]  # 重采样会放弃2018-10-31的样本，所以在此加上2018-11-01\nremote_rain_df.loc[pd.Timestamp('2018-11-01')\n] = remote_rain_df.loc[pd.Timestamp('2018-10-31')]  # 重采样会放弃2018-10-31 23:00的样本，所以在此加上2018-11-01\nremote_rain_df['rain_sum'] = remote_rain_df.sum(axis=1)  # 遥感数据求和\nremote_rain_resample = remote_rain_df[['rain_sum']].resample(rule='3H').sum()  # 遥感数据重采样\nrain_raw_resample = rain_raw_df.resample(rule='3H').interpolate('linear') / 8  # 雨量预测重采样\nenv_raw_resample = env_raw_df.resample(rule='3H').interpolate('linear')  # 环境重采样\nenv_rain = pd.concat(\n    (rain_raw_resample,\n     env_raw_resample,\n     remote_rain_resample),\n    axis=1)  # 所有重采样的数据合并\nenv_rain.tail(66)\n\nenv_rain.index = env_rain.index - pd.Timedelta(value=1, unit='H')  # 所有重采样的数据合并后，调整时间戳\n\n\n# resample and fillna 重采样，fillna\nrain_raw_df_final.loc[pd.Timestamp('2019-11-08')\n] = rain_raw_df_final.loc[pd.Timestamp('2019-11-07')]  # 重采样会放弃2018-11-07的样本，所以在此加上2018-11-08\nenv_raw_df_final.loc[pd.Timestamp('2019-11-01')\n] = env_raw_df_final.loc[pd.Timestamp('2019-10-31')]  # 重采样会放弃2018-10-31的样本，所以在此加上2018-11-01\nremote_rain_df_final.loc[pd.Timestamp('2019-11-01')\n] = remote_rain_df_final.loc[pd.Timestamp('2019-10-31')]  # 重采样会放弃2018-10-31 23:00的样本，所以在此加上2018-11-01\nremote_rain_df_final['rain_sum'] = remote_rain_df_final.sum(axis=1)  # 遥感数据求和\nremote_rain_resample_final = remote_rain_df_final[['rain_sum']].resample(rule='3H').sum()  # 遥感数据重采样\nrain_raw_resample_final = rain_raw_df_final.resample(rule='3H').interpolate('linear') / 8  # 雨量预测重采样\nenv_raw_resample_final = env_raw_df_final.resample(rule='3H').interpolate('linear')  # 环境重采样\nenv_rain_final = pd.concat(\n    (rain_raw_resample_final,\n     env_raw_resample_final,\n     remote_rain_resample_final),\n    axis=1)  # 所有重采样的数据合并\nenv_rain_final.tail(66)\n\nenv_rain_final.index = env_rain_final.index - pd.Timedelta(value=1, unit='H')  # 所有重采样的数据合并后，调整时间戳","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c3370fa-76f7-4736-8258-84d09b5f5272","_cell_guid":"77f6e06a-9f3f-4999-b256-680838c30cf3","trusted":true},"cell_type":"code","source":"raw_all_df = pd.concat((qi_raw_df, env_rain), axis=1)  # 合并重采样数据和流量\nraw_all_df_final = pd.concat((qi_raw_df_final, env_rain_final), axis=1)  # 合并重采样数据和流量\n\nraw_all_df = pd.concat((raw_all_df, raw_all_df_final), axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1b80b3a-b5cc-4722-99f1-f76538d44aea","_cell_guid":"7e8b2bf3-9544-4b89-98a7-17a3f9e936e7","trusted":true},"cell_type":"code","source":"pf_df = raw_all_df.copy()\n# predict rain_sum\nfrom sklearn.linear_model import LinearRegression\n\npf_df['rain_last'] = pf_df['rain_sum'].shift(1)\npf_df['D1_last'] = pf_df['D1'].shift(1)\ntrain_df = pf_df[['rain_sum', 'D1_last', 'rain_last']].dropna(axis=0).copy()\nX = train_df[['D1_last', 'rain_last']].values\ny = train_df['rain_sum'].values\nlr_rain = LinearRegression().fit(X, y)\ntest_df = pf_df[['D1_last', 'rain_last']].dropna(axis=0).copy()\n\ntrain_df['rain_pred'] = lr_rain.predict(X)\npf_df = pd.concat((pf_df, train_df[['rain_pred']]), axis=1)\n\n\ndef recursive_forecast(model, test_X, fill_col, X_cols, y_col, t_delta='3H', ):\n    test_X_1 = test_X.copy()\n    for next_x in test_X_1.index:\n        pred = model.predict(test_X_1.loc[[next_x], X_cols].values)\n        if next_x + pd.Timedelta(t_delta) <= test_X_1.index.max():\n            test_X_1.loc[[next_x + pd.Timedelta(t_delta)], [fill_col]] = pred\n    test_X_1[y_col] = model.predict(test_X_1[X_cols].values)\n    return test_X_1\n\n\n# print(pf_df['2018-11-01':'2018-11-07'])\npf_df.loc['2018-11-01':'2018-11-07'] = recursive_forecast(lr_rain, pf_df.loc['2018-11-01':'2018-11-07', ],\n                                                          fill_col='rain_last', X_cols=['D1_last', 'rain_last'],\n                                                          y_col='rain_pred', t_delta='3H', )\npf_df.loc['2018-02-01':'2018-02-07'] = recursive_forecast(lr_rain, pf_df.loc['2018-02-01':'2018-02-07', ],\n                                                          fill_col='rain_last', X_cols=['D1_last', 'rain_last'],\n                                                          y_col='rain_pred', t_delta='3H', )\npf_df.loc['2018-08-01':'2018-08-07'] = recursive_forecast(lr_rain, pf_df.loc['2018-08-01':'2018-08-07', ],\n                                                          fill_col='rain_last', X_cols=['D1_last', 'rain_last'],\n                                                          y_col='rain_pred', t_delta='3H', )\npf_df.loc['2019-02-01':'2019-02-07'] = recursive_forecast(lr_rain, pf_df.loc['2019-02-01':'2019-02-07', ],\n                                                          fill_col='rain_last', X_cols=['D1_last', 'rain_last'],\n                                                          y_col='rain_pred', t_delta='3H', )\npf_df.loc['2019-04-01':'2019-04-07'] = recursive_forecast(lr_rain, pf_df.loc['2019-04-01':'2019-04-07', ],\n                                                          fill_col='rain_last', X_cols=['D1_last', 'rain_last'],\n                                                          y_col='rain_pred', t_delta='3H', )\npf_df.loc['2019-06-01':'2019-06-07'] = recursive_forecast(lr_rain, pf_df.loc['2019-06-01':'2019-06-07', ],\n                                                          fill_col='rain_last', X_cols=['D1_last', 'rain_last'],\n                                                          y_col='rain_pred', t_delta='3H', )\npf_df.loc['2019-08-01':'2019-08-07'] = recursive_forecast(lr_rain, pf_df.loc['2019-08-01':'2019-08-07', ],\n                                                          fill_col='rain_last', X_cols=['D1_last', 'rain_last'],\n                                                          y_col='rain_pred', t_delta='3H', )\npf_df.loc['2019-11-01':'2019-11-07'] = recursive_forecast(lr_rain, pf_df.loc['2019-11-01':'2019-11-07', ],\n                                                          fill_col='rain_last', X_cols=['D1_last', 'rain_last'],\n                                                          y_col='rain_pred', t_delta='3H', )\n\n\n\n\nprint(pf_df.tail(60))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pf_df['rain_pred'].tail(60))\n# pf_df[pf_df[['rain_pred','rain_last']].isna()][['rain_pred']]\nprint(pf_df[['rain_pred']])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa877f07-b75f-4d73-bc10-69b7021af7bd","_cell_guid":"31a5d52f-fd8c-4934-ade3-ffba9e19fd16","trusted":true},"cell_type":"code","source":"# 采用prophet 预测\n# feature engineering\npf_df['future_5D_rain_sum'] = pf_df[[\n    'D1', 'D2', 'D3', 'D4', 'D5']].sum(axis=1)  # 预测数，5天求和\n# D1-D5, create long lag because it has data on test data set\npf_df[['D1_x_lag']] = pf_df[['D1']].rolling(102).mean()\npf_df[['D2_x_lag']] = pf_df[['D2']].rolling(110).mean()\npf_df[['D3_x_lag']] = pf_df[['D3']].rolling(108).mean()\npf_df[['D4_x_lag']] = pf_df[['D4']].rolling(123).mean()\npf_df[['D5_x_lag']] = pf_df[['D5']].rolling(134).mean()\npf_df['future_5D_rain_sum_x_lag'] = pf_df[['future_5D_rain_sum']].rolling(100).mean()\npf_df[['rain_pred_x_lag']] = pf_df[['rain_pred']].rolling('11D6H').mean()\n\ntarget = 'Qi'\n# cols = [\n#     target,\n#     'T',\n#     'rain_sum',\n#     'w',\n#     'wd','D1', 'D2', 'D3', 'D4', 'D5','future_5D_rain_sum'\n# ]\n# non_use_cols = ['ds'] + cols\n\n# def create_lags(df, cols, lag, base=1, suffix=None):\n#     if suffix is None:\n#         suffix = '_' + str(lag) + 'days'\n#     for col in cols:\n#         df[col + suffix] = df[col].shift(lag * base)\n#     return df\n\nprint(pf_df.tail(60))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20bb065c-1970-4e22-b5ef-d84099a6f46b","_cell_guid":"5ab99cff-a6fd-425c-a6bd-cdc409b373bd","trusted":true},"cell_type":"code","source":"def fillna_to_outliers(df, col, window='1D', threshold=0.0):\n    std = df[col].rolling(window=window).std()\n    std.fillna(0, inplace=True)\n    df.loc[df[std <= threshold].index, col] = None\n    return df\n\n\nprint(pf_df.isna().sum())\npf_df1 = fillna_to_outliers(pf_df, 'Qi', window='12H')\nprint(pf_df1.isna().sum())\nprint(pf_df1.tail())\npf_df1['Qi'].plot()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"410288f2-9c87-4dca-8550-51f2aaf3fd1c","_cell_guid":"acb0e45a-a149-496a-aeb3-c34037eaa6b5","trusted":true},"cell_type":"code","source":"pf_df['ds'] = pf_df.index  # prophet 需要命名ds列\npf_df.rename({'Qi': 'y'}, axis=1, inplace=True)  # prophet 需要命名独的y列\n\nprint(pf_df.columns)\npf_df = pf_df.sort_values(by=['ds'])\npf_df['cap'] = 0.9\npf_df['floor'] = 0.01\n\npf_df['on_season'] = (pf_df['ds'].dt.month > 3) & (pf_df['ds'].dt.month < 12)\npf_df['off_season'] = ~(pf_df['ds'].dt.month > 3) & (pf_df['ds'].dt.month < 12)\n\npf_df['D1_on'] = pf_df['on_season'] * pf_df['D1_x_lag']\npf_df['D1_off'] = pf_df['off_season'] * pf_df['D1_x_lag']\npf_df['D2_on'] = pf_df['on_season'] * pf_df['D2_x_lag']\npf_df['D2_off'] = pf_df['off_season'] * pf_df['D2_x_lag']\npf_df['D3_on'] = pf_df['on_season'] * pf_df['D3_x_lag']\npf_df['D3_off'] = pf_df['off_season'] * pf_df['D3_x_lag']\npf_df['D4_on'] = pf_df['on_season'] * pf_df['D4_x_lag']\npf_df['D4_off'] = pf_df['off_season'] * pf_df['D4_x_lag']\npf_df['D5_on'] = pf_df['on_season'] * pf_df['D5_x_lag']\npf_df['D5_off'] = pf_df['off_season'] * pf_df['D5_x_lag']\npf_df['rain_pred_on'] = pf_df['on_season'] * pf_df['rain_pred_x_lag']\npf_df['rain_pred_off'] = pf_df['off_season'] * pf_df['rain_pred_x_lag']\n\npf_df.describe()\nstart_dt = pd.Timestamp('2013-04-01 00:00:00')\nend_dt = pd.Timestamp('2019-11-07 23:00:00')\ntrain_df = pf_df[(pf_df['ds'] <= end_dt) & (pf_df['ds'] >= start_dt)]  # .dropna(axis=0)\n\n# pf_use_cols = ['ds','cap','floor','y','T','w','wd','rain_sum','D1','D2','D3','D4','D5','future_5D_rain_sum']\npf_use_cols = ['ds', 'cap', 'floor', 'y', 'D1_x_lag', 'D2_x_lag', 'D3_x_lag', 'D4_x_lag', 'D5_x_lag', 'on_season',\n               'off_season', 'D1_on', 'D2_on', 'D3_on', 'D4_on', 'D5_on', 'D1_off', 'D2_off', 'D3_off', 'D4_off',\n               'D5_off', 'rain_pred_on', 'rain_pred_off',\n               'rain_pred_x_lag']  # ,'future_5D_rain_sum','T_x_lag','w_x_lag','rain_sum_x_lag']\n# pf_use_cols = pf_df.columns\npf_model = Prophet(yearly_seasonality=False,\n                   weekly_seasonality=False,\n                   daily_seasonality=False, growth='logistic')\npf_model.add_regressor('D1_x_lag')\npf_model.add_regressor('D2_x_lag')\npf_model.add_regressor('D3_x_lag')\npf_model.add_regressor('D4_x_lag')\npf_model.add_regressor('D5_x_lag')\npf_model.add_regressor('rain_pred_x_lag')\npf_model.add_regressor('rain_pred_on')\npf_model.add_regressor('rain_pred_off')\npf_model.add_regressor('D1_on')\npf_model.add_regressor('D2_on')\npf_model.add_regressor('D3_on')\npf_model.add_regressor('D4_on')\npf_model.add_regressor('D5_on')\npf_model.add_regressor('D1_off')\npf_model.add_regressor('D2_off')\npf_model.add_regressor('D3_off')\npf_model.add_regressor('D4_off')\npf_model.add_regressor('D5_off')\npf_model.add_seasonality(name='year_on', period=365.25, fourier_order=24, prior_scale=10, condition_name='on_season')\npf_model.add_seasonality(name='year_off', period=365.25, fourier_order=24, prior_scale=10, condition_name='off_season')\n\npf_model.add_seasonality(name='week_on', period=7, fourier_order=24, prior_scale=10, condition_name='on_season')\npf_model.add_seasonality(name='week_off', period=7, fourier_order=24, prior_scale=10, condition_name='off_season')\n\npf_model.add_seasonality(name='day_on', period=1, fourier_order=24, prior_scale=10, condition_name='on_season')\npf_model.add_seasonality(name='day_off', period=1, fourier_order=24, prior_scale=10, condition_name='off_season')\n\npf_model.add_seasonality(name='month_on', period=30, fourier_order=24, prior_scale=10, condition_name='on_season')\npf_model.add_seasonality(name='month_off', period=30, fourier_order=24, prior_scale=10, condition_name='off_season')\npf_model.add_seasonality(name='6_month_on', period=182.625, fourier_order=24, prior_scale=10,\n                         condition_name='on_season')\npf_model.add_seasonality(name='6_month_off', period=182.625, fourier_order=24, prior_scale=10,\n                         condition_name='off_season')\n\npf_model.fit(train_df[pf_use_cols])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aedcb7a9-8f89-4c46-b016-b070f131a472","_cell_guid":"4511ca5f-fe52-404c-ad3c-c77cba994d00","trusted":true},"cell_type":"code","source":"# 将prophet的预测作为一个特征，后面决定是否需要采用，接下来需要采用lightgbm\nlgb_df = pf_df.copy()\npf_check_nan_cols = pf_use_cols[4:]\npf_use_cols[4:]\npf_predict_all_df = pf_df[pf_check_nan_cols].dropna(axis=0)\ny_hat = pf_model.predict(lgb_df.loc[pf_predict_all_df.index])\ny_hat.set_index('ds', inplace=True)\nlgb_df.rename({'y': 'Qi'}, axis=1, inplace=True)\nlgb_df = pd.concat((lgb_df, y_hat[['yhat']]), axis=1)\nprint(lgb_df['yhat'].describe())\nprint(lgb_df['yhat'].tail(66))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d950e93-9860-4fd4-8c04-819f100e0a48","_cell_guid":"d398cde9-89cf-4931-990a-d239f3813dc2","trusted":true},"cell_type":"code","source":"lgb_df['yhat'][lgb_df['yhat'] <= 0.001] = 0.001\nlgb_df['yhat'][lgb_df['yhat'] >= 0.99] = 0.99","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63adb07e-59e1-4126-b0f9-6a0d9540d291","_cell_guid":"dae70375-1cc1-4531-8f46-6233b01d5ce2","trusted":true},"cell_type":"code","source":"fg, ax = plt.subplots()\n# ax.plot(lgb_df.loc['2013-10':'2014-02',['D1_x_lag']].values)\n# ax.plot(lgb_df.loc['2014-10':'2015-02',['D1_x_lag']].values)\n# ax.plot(lgb_df.loc['2015-10':'2016-02',['D1_x_lag']].values)\nax.plot(lgb_df.loc['2016-10':'2017-02', ['D1_x_lag']].values)\n\nplt.show()\nax.set_ylim(0, 0.5)\n\nfg, ax1 = plt.subplots()\n# ax1.plot(lgb_df.loc['2013-10':'2014-02',['Qi']].values)\n# ax1.plot(lgb_df.loc['2014-10':'2015-02',['Qi']].values)\n# ax1.plot(lgb_df.loc['2015-10':'2016-02',['Qi']].values)\nax1.plot(lgb_df.loc['2016-10':'2017-02', ['Qi']])\n\nplt.show()\n\n\n# print(lgb_df.head(10))\n# print(lgb_df.tail(10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8370147b-b7c2-4a49-b653-22707e4877a4","_cell_guid":"a71ddb42-ab3e-4556-ace9-570c576ca044","trusted":true},"cell_type":"code","source":"def nes_score1(y_ture, y_pred):\n    nes_score = 1 - np.sum((y_ture - y_pred) ** 2) / np.sum((y_ture - np.mean(y_ture)) ** 2)\n    score = nes_score * 100\n    return score\n\n\nr = 10\nr_s = lgb_df[['Qi', 'yhat']].copy().dropna(axis=0)\n\nplt.plot(r_s['yhat'].values[-56:])\nplt.plot(r_s['Qi'].values[-56:])\nprint(nes_score1(r_s['Qi'].values, r_s['yhat'].values))\nprint('01')\nprint(nes_score1(r_s.loc['2014-01', 'Qi'].values, r_s.loc['2014-01', 'yhat'].values))\nprint(nes_score1(r_s.loc['2015-01', 'Qi'].values, r_s.loc['2015-01', 'yhat'].values))\nprint(nes_score1(r_s.loc['2016-01', 'Qi'].values, r_s.loc['2016-01', 'yhat'].values))\nprint(nes_score1(r_s.loc['2017-01', 'Qi'].values, r_s.loc['2017-01', 'yhat'].values))\nprint(nes_score1(r_s.loc['2018-01', 'Qi'].values, r_s.loc['2018-01', 'yhat'].values))\nprint(nes_score1(r_s.loc['2019-01', 'Qi'].values, r_s.loc['2019-01', 'yhat'].values))\n\nprint('02')\nprint(nes_score1(r_s.loc['2014-02', 'Qi'].values, r_s.loc['2014-02', 'yhat'].values))\nprint(nes_score1(r_s.loc['2015-02', 'Qi'].values, r_s.loc['2015-02', 'yhat'].values))\nprint(nes_score1(r_s.loc['2016-02', 'Qi'].values, r_s.loc['2016-02', 'yhat'].values))\nprint(nes_score1(r_s.loc['2017-02', 'Qi'].values, r_s.loc['2017-02', 'yhat'].values))\n\nprint('03')\nprint(nes_score1(r_s.loc['2014-03', 'Qi'].values, r_s.loc['2014-03', 'yhat'].values))\nprint(nes_score1(r_s.loc['2015-03', 'Qi'].values, r_s.loc['2015-03', 'yhat'].values))\nprint(nes_score1(r_s.loc['2016-03', 'Qi'].values, r_s.loc['2016-03', 'yhat'].values))\nprint(nes_score1(r_s.loc['2017-03', 'Qi'].values, r_s.loc['2017-03', 'yhat'].values))\nprint('04')\nprint(nes_score1(r_s.loc['2014-04', 'Qi'].values, r_s.loc['2014-04', 'yhat'].values))\nprint(nes_score1(r_s.loc['2015-04', 'Qi'].values, r_s.loc['2015-04', 'yhat'].values))\nprint(nes_score1(r_s.loc['2016-04', 'Qi'].values, r_s.loc['2016-04', 'yhat'].values))\nprint(nes_score1(r_s.loc['2017-04', 'Qi'].values, r_s.loc['2017-04', 'yhat'].values))\n\n\nprint('05')\nprint(nes_score1(r_s.loc['2014-05', 'Qi'].values, r_s.loc['2014-05', 'yhat'].values))\nprint(nes_score1(r_s.loc['2015-05', 'Qi'].values, r_s.loc['2015-05', 'yhat'].values))\nprint(nes_score1(r_s.loc['2016-05', 'Qi'].values, r_s.loc['2016-05', 'yhat'].values))\nprint(nes_score1(r_s.loc['2017-05', 'Qi'].values, r_s.loc['2017-05', 'yhat'].values))\nprint(nes_score1(r_s.loc['2019-05', 'Qi'].values, r_s.loc['2019-05', 'yhat'].values))\nprint('06')\n\nprint(nes_score1(r_s.loc['2014-06', 'Qi'].values, r_s.loc['2014-06', 'yhat'].values))\nprint(nes_score1(r_s.loc['2015-06', 'Qi'].values, r_s.loc['2015-06', 'yhat'].values))\nprint(nes_score1(r_s.loc['2016-06', 'Qi'].values, r_s.loc['2016-06', 'yhat'].values))\nprint(nes_score1(r_s.loc['2017-06', 'Qi'].values, r_s.loc['2017-06', 'yhat'].values))\nprint('07')\nprint(nes_score1(r_s.loc['2014-07', 'Qi'].values, r_s.loc['2014-07', 'yhat'].values))\nprint(nes_score1(r_s.loc['2015-07', 'Qi'].values, r_s.loc['2015-07', 'yhat'].values))\nprint(nes_score1(r_s.loc['2016-07', 'Qi'].values, r_s.loc['2016-07', 'yhat'].values))\nprint(nes_score1(r_s.loc['2017-07', 'Qi'].values, r_s.loc['2017-07', 'yhat'].values))\nprint(nes_score1(r_s.loc['2018-07', 'Qi'].values, r_s.loc['2018-07', 'yhat'].values))\nprint(nes_score1(r_s.loc['2019-07', 'Qi'].values, r_s.loc['2019-07', 'yhat'].values))\n\nprint(nes_score1(r_s.loc['2014-08', 'Qi'].values, r_s.loc['2014-08', 'yhat'].values))\nprint(nes_score1(r_s.loc['2015-08', 'Qi'].values, r_s.loc['2015-08', 'yhat'].values))\nprint(nes_score1(r_s.loc['2016-08', 'Qi'].values, r_s.loc['2016-08', 'yhat'].values))\nprint(nes_score1(r_s.loc['2017-08', 'Qi'].values, r_s.loc['2017-08', 'yhat'].values))\n\nprint('10')\nprint(nes_score1(r_s.loc['2014-10', 'Qi'].values, r_s.loc['2014-10', 'yhat'].values))\nprint(nes_score1(r_s.loc['2015-10', 'Qi'].values, r_s.loc['2015-10', 'yhat'].values))\nprint(nes_score1(r_s.loc['2016-10', 'Qi'].values, r_s.loc['2016-10', 'yhat'].values))\nprint(nes_score1(r_s.loc['2017-10', 'Qi'].values, r_s.loc['2017-10', 'yhat'].values))\nprint(nes_score1(r_s.loc['2018-10', 'Qi'].values, r_s.loc['2018-10', 'yhat'].values))\nprint('11')\nprint(nes_score1(r_s.loc['2014-11', 'Qi'].values, r_s.loc['2014-11', 'yhat'].values))\nprint(nes_score1(r_s.loc['2015-11', 'Qi'].values, r_s.loc['2015-11', 'yhat'].values))\nprint(nes_score1(r_s.loc['2016-11', 'Qi'].values, r_s.loc['2016-11', 'yhat'].values))\nprint(nes_score1(r_s.loc['2017-11', 'Qi'].values, r_s.loc['2017-11', 'yhat'].values))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04897821-9b7c-45cc-bdf8-dd49e6704334","_cell_guid":"fa8ddee8-fd9e-4b60-ad32-73ecb682df05","trusted":true},"cell_type":"code","source":"test_y_hat_1 = lgb_df.loc['2018-02-01':'2018-02-07', ['yhat']].values\ntest_y_hat_2 = lgb_df.loc['2018-08-01':'2018-08-07', ['yhat']].values\ntest_y_hat_3 = lgb_df.loc['2018-11-01':'2018-11-07', ['yhat']].values\n\ntest_y_hat_4 = lgb_df.loc['2019-02-01':'2019-02-07', ['yhat']].values\ntest_y_hat_5 = lgb_df.loc['2019-04-01':'2019-04-07', ['yhat']].values\ntest_y_hat_6 = lgb_df.loc['2019-06-01':'2019-06-07', ['yhat']].values\ntest_y_hat_7 = lgb_df.loc['2019-08-01':'2019-08-07', ['yhat']].values\ntest_y_hat_8 = lgb_df.loc['2019-11-01':'2019-11-07', ['yhat']].values\n#plt.plot(test_y_hat_1)\n#plt.plot(test_y_hat_2)\n#plt.plot(test_y_hat_3)\nplt.plot(test_y_hat_4)\nplt.plot(test_y_hat_5)\nplt.plot(test_y_hat_6)\nplt.plot(test_y_hat_7)\nplt.plot(test_y_hat_8)\n# submit\nSubmitRaw = data_file + '/submission.csv'\nsubmit_empty = pd.read_csv(SubmitRaw, index_col=0)\n\ntest_all_submission = np.vstack((test_y_hat_1.T, test_y_hat_2.T, test_y_hat_3.T))\nsubmit_df = pd.DataFrame(test_all_submission, columns=submit_empty.columns)\nsubmit_df.to_csv(\"./submission_fb.csv\")\n\ntest_all_submission_final = np.vstack((test_y_hat_4.T, test_y_hat_5.T, test_y_hat_6.T, test_y_hat_7.T, test_y_hat_8.T))\nsubmit_df_final = pd.DataFrame(test_all_submission_final, columns=submit_empty.columns)\nsubmit_df_final.to_csv(\"./submission_fb_final.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba1d58f0-ced4-4f90-a4c8-2a97dec3d548","_cell_guid":"d5e086c4-4436-4347-a27d-a1f0b598a390","trusted":true},"cell_type":"code","source":"lgb_df[['Qi', 'yhat']].plot()\nprint(lgb_df.tail(10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ff04206-fbad-4d1a-8ca8-7ecb065c2282","_cell_guid":"2ebdb871-08df-49e5-be31-f93b587e3315","trusted":true},"cell_type":"code","source":"# engineering for lightgbm lightgbm t特征工程\n# 平方和开方\n\n\n# # for below features ,we have to shift, because the last 7 days has no data\n# pf_df[['T_x_lag']] =pf_df[['T']].rolling(99).mean().shift(56) #we have\n# pf_df[['w_x_lag']] =pf_df[['w']].rolling(65).mean().shift(56)\n\n# #！ wd corr is too small\n# lgb_df['T_x_lag_p'] = lgb_df['T_x_lag']**2\n# lgb_df['T_x_lag_s'] = lgb_df['T_x_lag']**(1 / 2)\n# lgb_df['rain_sum_x_lag_p'] = lgb_df['rain_sum_x_lag']**2\n# lgb_df['rain_sum_x_lag_s'] = lgb_df['rain_sum_x_lag']**(1 / 2)\n# lgb_df['w_x_lag_p'] = lgb_df['w_x_lag']**2\n# lgb_df['w_x_lag_s'] = lgb_df['w_x_lag']**(1 / 2)\n# lgb_df['future_5D_rain_sum_x_lag_p'] = lgb_df['future_5D_rain_sum_x_lag']**2\n# lgb_df['future_5D_rain_sum_x_lag_s'] = lgb_df['future_5D_rain_sum_x_lag']**(1 / 2)\n\n# lgb_df['w_rain_x_lag']=lgb_df['rain_sum_x_lag']*lgb_df['w_x_lag']\n# lgb_df['w_T_x_lag']=lgb_df['T_x_lag']*lgb_df['w_x_lag']\n# lgb_df['T_rain_x_lag']=lgb_df['rain_sum_x_lag']*lgb_df['T_x_lag']\n\n\ntarget = 'Qi'  # 目标\ncols = [\n    'Qi',\n    'T',\n    'rain_sum',\n    'w',\n    'wd',\n    'D1', 'D2', 'D3', 'D4', 'D5', 'future_5D_rain_sum'  # 以下列需要创建lag\n]\nnon_use_cols = ['ds', target] + cols  # 这些列无法放入特征，因为预测当天没有这些数据\n\n\ndef create_lags(df, cols, lag, base=1, suffix=None):\n    if suffix is None:\n        suffix = '_' + str(lag) + 'days'\n    for col in cols:\n        df[col + suffix] = df[col].shift(lag * base)\n    return df\n\n\nlgb_df = create_lags(\n    lgb_df,\n    ['Qi'],\n    lag=1,\n    base=8,\n    suffix=None)  # create 1 days lag  如果创建了小于7天的lag，那么需要用滚动预测\n\nlgb_df = create_lags(\n    lgb_df,\n    ['Qi'],\n    lag=1,\n    base=1,\n    suffix='3H')  # create 3H lag 如果创建了小于7天的lag，那么需要用滚动预测\n\n\n# lgb_df = create_lags(\n#     lgb_df,\n#     cols,\n#     lag=5,\n#     base=8,\n#     suffix=None)  # create 5 days lag\n\n# lgb_df = create_lags(\n#     lgb_df,\n#     cols,\n#     lag=7,\n#     base=8,\n#     suffix=None)  # create 7 days lag\n# lgb_df = create_lags(lgb_df, cols, lag=30, base=8,\n#                      suffix=None)  # create 30 days lag\n\n# create time series period features #时间相关的特征\ndef time_period_features(df, time_col):\n    # in hour level\n    df['hour'] = df[time_col].dt.hour\n    # in day level\n    df['days_in_month'] = df[time_col].dt.days_in_month\n    df['dayofyear'] = df[time_col].dt.dayofyear\n    # week levels\n    df['weekofyear'] = df[time_col].dt.weekofyear\n    # month level\n    df['month'] = df[time_col].dt.month\n    # quarter level\n    df['quarter'] = df[time_col].dt.quarter\n    # year level\n    df['year'] = df[time_col].dt.year\n    return df\n\n\nprint(lgb_df.columns)\n\nlgb_df = time_period_features(lgb_df, 'ds')\n\n# #create window period features 滚动窗口特征，这里只有均值（有些求和），可能需要加std,median 等\n# lgb_df[['T_7days_mean',\n#         'rain_sum_7days_sum',\n#         'w_7days_mean',\n#         'wd_7days_mean',\n#         'T_p_7days_mean',\n#         'rain_sum_p_7days_sum',\n#         'w_p_7days_mean',\n#         'T_s_7days_mean',\n#         'rain_sum__s_7days_sum',\n#         'w_s_7days_mean',\n#         ]] = lgb_df.rolling('12D').agg({'T_5days': np.mean,\n#                                         'rain_sum_5days': np.sum,\n#                                         'w_5days': np.mean,\n#                                         'wd_5days': np.mean,\n#                                         'T_p_5days': np.mean,\n#                                         'rain_sum_p_5days': np.sum,\n#                                         'w_p_5days': np.mean,\n#                                         'T_s_5days': np.mean,\n#                                         'rain_sum_s_5days': np.sum,\n#                                         'w_s_5days': np.mean,\n#                                         }).shift(12 * 8)\n# lgb_df[['T_30days_mean',\n#         'rain_sum_30days_sum',\n#         'w_30days_mean',\n#         'wd_30days_mean',\n#         'T_p_30days_mean',\n#         'rain_sum_p_30days_sum',\n#         'w_p_30days_mean',\n#         'T_s_30days_mean',\n#         'rain_sum__s_30days_sum',\n#         'w_s_30days_mean',\n#         ]] = lgb_df.rolling('35D').agg({'T_5days': np.mean,\n#                                         'rain_sum_5days': np.sum,\n#                                         'w_5days': np.mean,\n#                                         'wd_5days': np.mean,\n#                                         'T_p_5days': np.mean,\n#                                         'rain_sum_p_5days': np.sum,\n#                                         'w_p_5days': np.mean,\n#                                         'T_s_5days': np.mean,\n#                                         'rain_sum_s_5days': np.sum,\n#                                         'w_s_5days': np.mean,\n#                                         }).shift(30 * 8)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca0fab41-5a27-49d9-8924-1457574f3758","_cell_guid":"bff94bc5-f814-4e4a-baef-de046b60dcbb","trusted":true},"cell_type":"code","source":"lgb_df.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e5a31c3-9886-463c-bcb4-6b3a1b400e5c","_cell_guid":"9cf7383c-cf27-4898-bdda-06c4712b707d","trusted":true},"cell_type":"code","source":"lgb_df[['D1_x_lag_std']] = lgb_df[['D1']].rolling(102).std()\nlgb_df[['D2_x_lag_std']] = lgb_df[['D2']].rolling(110).std()\nlgb_df[['D3_x_lag_std']] = lgb_df[['D3']].rolling(108).std()\nlgb_df[['D4_x_lag_std']] = lgb_df[['D4']].rolling(123).std()\nlgb_df[['D5_x_lag_std']] = lgb_df[['D5']].rolling(134).std()\nlgb_df['future_5D_rain_sum_x_lag_std'] = lgb_df[['future_5D_rain_sum']].rolling(100).std()\nlgb_df[['rain_pred_x_lag_std']] = lgb_df[['rain_pred']].rolling('11D6H').std()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c18cfedd-319e-4b26-93e5-63982ac6652b","_cell_guid":"041df252-e971-402f-9f8e-574e4b736b1f","trusted":true},"cell_type":"code","source":"# rolling_cols = ['Qi3H']\n# lgb_df[[c +'_30d_std' for c in rolling_cols]] =lgb_df[rolling_cols].rolling('30D3H').std().shift(30 * 8)\n# lgb_df[[c +'_7d_std' for c in rolling_cols]] =lgb_df[rolling_cols].rolling('7D3H').std().shift(7 * 8)\n# lgb_df[[c +'_1d_std' for c in rolling_cols]] =lgb_df[rolling_cols].rolling('1D3H').std().shift(1 * 8)\n# lgb_df[[c +'_30d_median' for c in rolling_cols]] =lgb_df[rolling_cols].rolling('35D').median().shift(30 * 8)\n# lgb_df[[c +'_7d_median' for c in rolling_cols]] =lgb_df[rolling_cols].rolling('12D').median().shift(7 * 8)\n# lgb_df[[c +'_30d_max' for c in rolling_cols]] =lgb_df[rolling_cols].rolling('35D').max().shift(30 * 8)\n# lgb_df[[c +'_7d_max' for c in rolling_cols]] =lgb_df[rolling_cols].rolling('12D').max().shift(7 * 8)\n# lgb_df[[c +'_30d_min' for c in rolling_cols]] =lgb_df[rolling_cols].rolling('35D').min().shift(30 * 8)\n# lgb_df[[c +'_7d_min' for c in rolling_cols]] =lgb_df[rolling_cols].rolling('12D').min().shift(7 * 8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_df['2016-12'].isna().sum() # 训练集\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b266265e-0a9c-4e79-8373-b7bf760947d1","_cell_guid":"4d6f284f-30e9-4449-a850-166af525e8af","trusted":true},"cell_type":"code","source":"lgb_use_cols = ['rain_pred', 'D1_x_lag',\n                'D2_x_lag', 'D3_x_lag', 'D4_x_lag', 'D5_x_lag',\n                'future_5D_rain_sum_x_lag', 'rain_pred_x_lag',\n                'on_season', 'off_season', 'D1_on', 'D1_off', 'D2_on', 'D2_off',\n                'D3_on', 'D3_off', 'D4_on', 'D4_off', 'D5_on', 'D5_off', 'rain_pred_on',\n                'rain_pred_off', 'yhat', 'D1_x_lag_std', 'D2_x_lag_std', 'D3_x_lag_std',\n                'D4_x_lag_std', 'D5_x_lag_std', 'future_5D_rain_sum_x_lag_std',\n                'rain_pred_x_lag_std', 'Qi_1days', 'Qi3H', 'hour', 'days_in_month',\n                'dayofyear', 'weekofyear', 'month', 'quarter', 'year', 'Qi_diff']\nnon_use_cols = [x for x in list(lgb_df.columns) if x not in lgb_use_cols]\n# split_train/test\ntrain_df = lgb_df['2013-01':'2016-12'].copy()  # 训练集\nvalidate_df = lgb_df['2017-01':'2019-11'].copy()  # 验证集\ntest_df_1 = lgb_df['2018-02-01':'2018-02-07']  # 测试1\ntest_df_2 = lgb_df['2018-08-01':'2018-08-07']  # 测试2\ntest_df_3 = lgb_df['2018-11-01':'2018-11-07']  # 测试3\ntest_df_4 = lgb_df['2019-02-01':'2019-02-07']  # 测试1\ntest_df_5 = lgb_df['2019-04-01':'2019-04-07']  # 测试2\ntest_df_6 = lgb_df['2019-06-01':'2019-06-07']  # 测试3\ntest_df_7 = lgb_df['2019-08-01':'2019-08-07']  # 测试2\ntest_df_8 = lgb_df['2019-11-01':'2019-11-07']  # 测试3\nprint(non_use_cols)\nsubmit_train_df = lgb_df['2013-01':'2019-11'].copy()  # 合并训练集和验证集，用于最终提交时训练的数据\n\n\ndef remove_calirate_samples_by_col(df, col, window='1D', threshold=0.0):\n    std = df[col].rolling(window=window).std()\n    std.fillna(0, inplace=True)\n    df = df[std > threshold]\n    return df\n\n\ntrain_df = remove_calirate_samples_by_col(train_df, 'Qi')\nvalidate_df = remove_calirate_samples_by_col(validate_df, 'Qi')\nsubmit_train_df = remove_calirate_samples_by_col(submit_train_df, 'Qi')\n\nsubmit_train_df.dropna(axis=0, inplace=True)\nsubmit_train_X = submit_train_df.drop(non_use_cols, axis=1)\nsubmit_train_y = submit_train_df[target]\n\ntrain_df.dropna(axis=0, inplace=True)  # 去掉包含nan的行（样本）\ntrain_X = train_df.drop(non_use_cols, axis=1)\ntrain_y = train_df[target]\nvalidate_df.dropna(axis=0, inplace=True)  # 去掉包含nan的行（样本）\nvalidate_X = validate_df.drop(non_use_cols, axis=1)\nvalidate_y = validate_df[target]\n\nprint(submit_train_X.tail(66))\nprint(submit_train_X.shape)\n\nprint(submit_train_X.columns)\n\n\ndef nes_metric(y_ture, y_pred):  # 定义nes ，这里没有采用权重，是因为验证的数据集不一定是56个时间戳\n    if isinstance(y_pred, lgb.Dataset):\n        y_pred = y_pred.get_label()\n    nes_score = 1 - np.sum((y_ture - y_pred) ** 2) / \\\n                np.sum((y_ture - np.mean(y_ture)) ** 2)\n    return 'nes_score', nes_score * 100, True\n\n\ntest_X_1 = test_df_1.drop(non_use_cols, axis=1)\ntest_X_2 = test_df_2.drop(non_use_cols, axis=1)\ntest_X_3 = test_df_3.drop(non_use_cols, axis=1)\ntest_X_4 = test_df_4.drop(non_use_cols, axis=1)\ntest_X_5 = test_df_5.drop(non_use_cols, axis=1)\ntest_X_6 = test_df_6.drop(non_use_cols, axis=1)\ntest_X_7 = test_df_7.drop(non_use_cols, axis=1)\ntest_X_8 = test_df_8.drop(non_use_cols, axis=1)\n\nprint(test_X_1['yhat'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05b8c670-42c1-4a53-9d58-90e8f9fc4927","_cell_guid":"de26977b-4d6a-4f09-a7b3-d20c2ee346be","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(random_state=0, n_jobs=-1)\nrf.fit(submit_train_X.values, submit_train_y.values)\nimportances = rf.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22d90792-4656-4d6f-ab0b-aa9254d40cd4","_cell_guid":"0ee7a2fc-189a-44e0-a855-8461a0cc9103","trusted":true},"cell_type":"code","source":"importance_s = pd.Series(importances, index=submit_train_X.columns)\nimportance_s.sort_values(ascending=False, inplace=True)\nimportance_s.head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6debe537-c1bf-4220-8fe5-4a2f3d37051c","_cell_guid":"8e143a62-f6f0-4fd3-8e36-f95d5815fbfd","trusted":true},"cell_type":"code","source":"# initial trial\ndtrain = lgb.Dataset(train_X.values, label=train_y.values)\ndvalidate = lgb.Dataset(validate_X.values, label=validate_y.values)\ndtrain_submit = lgb.Dataset(submit_train_X.values, label=submit_train_y.values)\nparam = {\n    'max_depth': 5,\n    'num_leaves': 30,\n    'eta': 0.05,\n    'objective': 'regression',\n    \"sub_row\": 0.5,\n    \"bagging_freq\": 1,\n    \"lambda_l2\": 0.3,\n    'verbose': 0,\n    'metric': None,\n    'boosting': 'dart'\n}\nevals_result = {}\nvalid_sets = [dtrain, dvalidate]\nvalid_name = ['train', 'eval']\nfeature_name = list(train_X.columns)\n\ncv_results = lgb.cv(  # kfold 验证\n    param,\n    dtrain_submit,\n    num_boost_round=515,\n    feature_name=feature_name,\n    feval=nes_metric,\n    stratified=False,\n    verbose_eval=True,\n    shuffle=False,\n    nfold=5)\n\nlgb_model = lgb.train(\n    param,\n    dtrain_submit,\n    num_boost_round=515,\n    feature_name=feature_name,\n    feval=nes_metric,\n    #valid_sets=valid_sets,\n    #valid_names=valid_name,\n    evals_result=evals_result)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48257bbc-2ca5-4ddc-b163-29fc6048d485","_cell_guid":"a185887e-76f4-401c-a1a8-5ee766c8bdaa","trusted":true},"cell_type":"code","source":"print(cv_results.keys())\nfig, ax = plt.subplots()\nax.plot(cv_results['nes_score-mean'])\nax.plot(cv_results['nes_score-stdv'])\nax.set_ylim(0, 100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"612d30e4-48f1-4760-8e40-689dc2c874e7","_cell_guid":"c36d2bd0-37ce-445e-aec6-6dfc87df8c3d","trusted":true},"cell_type":"code","source":"def recursive_forecast(model,test_X):\n    test_X_1 = test_X.copy()\n    for next_x in test_X_1.index:\n        pred = model.predict(test_X_1.loc[[next_x]])\n        if next_x + pd.Timedelta('3H') <= test_X_1.index.max():\n            test_X_1.loc[[next_x + pd.Timedelta('3H')], ['Qi3H']] = pred\n        if next_x + pd.Timedelta('1D') <= test_X_1.index.max():\n            test_X_1.loc[[next_x + pd.Timedelta('1D')], ['Qi_1days']] = pred\n    #         if next_x + pd.Timedelta('5D') <= test_X_1.index.max():\n    #             test_X_1.loc[[next_x + pd.Timedelta('5D')], ['Qi_5days']] = pred\n    return model.predict(test_X_1)\n\n\nvalie_y_hat_1 = lgb_model.predict(validate_X)\nplt.plot(valie_y_hat_1)\nplt.plot(validate_y.values)\nnes_metric(validate_y.values, valie_y_hat_1)\n\nvalie_y_hat_1 = lgb_model.predict(validate_X)\nplt.plot(valie_y_hat_1)\nplt.plot(validate_y.values)\nnes_metric(validate_y.values, valie_y_hat_1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10f62721-7fe3-4b3f-bcbb-572b45564a3b","_cell_guid":"7873d00f-01d9-4458-9426-4e962d968c2e","trusted":true},"cell_type":"code","source":"def nes_score1(y_ture, y_pred):\n    nes_score = 1 - np.sum((y_ture - y_pred) ** 2) / np.sum((y_ture - np.mean(y_ture)) ** 2)\n    score = nes_score * 100\n    return score\n\n\nlgb_y = lgb_model.predict(submit_train_X.values)\nlgb_y_df = pd.DataFrame(lgb_y, columns=['lgb_yhat'], index=submit_train_y.index)\nif 'lgb_yhat' in lgb_df.columns:\n    lgb_df['lgb_yhat'] = lgb_y_df['lgb_yhat']\nelse:\n    lgb_df = pd.concat([lgb_df, lgb_y_df], axis=1)\nr_s = lgb_df[['Qi', 'lgb_yhat']].copy().dropna(axis=0)\nr_s = r_s.loc[:, ~r_s.columns.duplicated(keep='last')]\n\nplt.plot(r_s['lgb_yhat'].values[-56:])\nplt.plot(r_s['Qi'].values[-56:])\nprint(nes_score1(r_s['Qi'].values, r_s['lgb_yhat'].values))\nprint('01')\nprint(nes_score1(r_s.loc['2014-01', 'Qi'].values, r_s.loc['2014-01', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2015-01', 'Qi'].values, r_s.loc['2015-01', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2016-01', 'Qi'].values, r_s.loc['2016-01', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2017-01', 'Qi'].values, r_s.loc['2017-01', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2018-01', 'Qi'].values, r_s.loc['2018-01', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2019-01', 'Qi'].values, r_s.loc['2019-01', 'lgb_yhat'].values))\n\nprint('02')\nprint(nes_score1(r_s.loc['2014-02', 'Qi'].values, r_s.loc['2014-02', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2015-02', 'Qi'].values, r_s.loc['2015-02', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2016-02', 'Qi'].values, r_s.loc['2016-02', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2017-02', 'Qi'].values, r_s.loc['2017-02', 'lgb_yhat'].values))\n\nprint('03')\nprint(nes_score1(r_s.loc['2014-03', 'Qi'].values, r_s.loc['2014-03', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2015-03', 'Qi'].values, r_s.loc['2015-03', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2016-03', 'Qi'].values, r_s.loc['2016-03', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2017-03', 'Qi'].values, r_s.loc['2017-03', 'lgb_yhat'].values))\nprint('04')\nprint(nes_score1(r_s.loc['2014-04', 'Qi'].values, r_s.loc['2014-04', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2015-04', 'Qi'].values, r_s.loc['2015-04', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2016-04', 'Qi'].values, r_s.loc['2016-04', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2017-04', 'Qi'].values, r_s.loc['2017-04', 'lgb_yhat'].values))\n\n\nprint('05')\nprint(nes_score1(r_s.loc['2014-05', 'Qi'].values, r_s.loc['2014-05', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2015-05', 'Qi'].values, r_s.loc['2015-05', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2016-05', 'Qi'].values, r_s.loc['2016-05', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2017-05', 'Qi'].values, r_s.loc['2017-05', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2019-05', 'Qi'].values, r_s.loc['2019-05', 'lgb_yhat'].values))\nprint('06')\n\nprint(nes_score1(r_s.loc['2014-06', 'Qi'].values, r_s.loc['2014-06', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2015-06', 'Qi'].values, r_s.loc['2015-06', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2016-06', 'Qi'].values, r_s.loc['2016-06', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2017-06', 'Qi'].values, r_s.loc['2017-06', 'lgb_yhat'].values))\nprint('07')\nprint(nes_score1(r_s.loc['2014-07', 'Qi'].values, r_s.loc['2014-07', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2015-07', 'Qi'].values, r_s.loc['2015-07', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2016-07', 'Qi'].values, r_s.loc['2016-07', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2017-07', 'Qi'].values, r_s.loc['2017-07', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2018-07', 'Qi'].values, r_s.loc['2018-07', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2019-07', 'Qi'].values, r_s.loc['2019-07', 'lgb_yhat'].values))\n\nprint(nes_score1(r_s.loc['2014-08', 'Qi'].values, r_s.loc['2014-08', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2015-08', 'Qi'].values, r_s.loc['2015-08', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2016-08', 'Qi'].values, r_s.loc['2016-08', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2017-08', 'Qi'].values, r_s.loc['2017-08', 'lgb_yhat'].values))\n\nprint('10')\nprint(nes_score1(r_s.loc['2014-10', 'Qi'].values, r_s.loc['2014-10', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2015-10', 'Qi'].values, r_s.loc['2015-10', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2016-10', 'Qi'].values, r_s.loc['2016-10', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2017-10', 'Qi'].values, r_s.loc['2017-10', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2018-10', 'Qi'].values, r_s.loc['2018-10', 'lgb_yhat'].values))\nprint('11')\nprint(nes_score1(r_s.loc['2014-11', 'Qi'].values, r_s.loc['2014-11', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2015-11', 'Qi'].values, r_s.loc['2015-11', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2016-11', 'Qi'].values, r_s.loc['2016-11', 'lgb_yhat'].values))\nprint(nes_score1(r_s.loc['2017-11', 'Qi'].values, r_s.loc['2017-11', 'lgb_yhat'].values))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7dafc3a-dd6b-45b6-bd6b-54425e142744","_cell_guid":"8e140562-9b63-4b60-87db-4f3605ef6cb6","trusted":true},"cell_type":"code","source":"print('01')\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2014-01'])\nprint(nes_metric(train_y['2014-01'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2015-01'])\nprint(nes_metric(train_y['2015-01'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2016-01'])\nprint(nes_metric(train_y['2016-01'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,validate_X['2017-01'])\nprint(nes_metric(validate_y['2017-01'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,validate_X['2018-01'])\nprint(nes_metric(validate_y['2018-01'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,validate_X['2019-01'])\nprint(nes_metric(validate_y['2019-01'].values, train_y_hat))\nprint('02')\n\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2014-02'])\nprint(nes_metric(train_y['2014-02'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2015-02'])\nprint(nes_metric(train_y['2015-02'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2016-02'])\nprint(nes_metric(train_y['2016-02'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,validate_X['2017-02'])\nprint(nes_metric(validate_y['2017-02'].values, train_y_hat))\n\n\nprint('03')\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2014-03'])\nprint(nes_metric(train_y['2014-03'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2015-03'])\nprint(nes_metric(train_y['2015-03'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2016-03'])\nprint(nes_metric(train_y['2016-03'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,validate_X['2017-03'])\nprint(nes_metric(validate_y['2017-03'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,validate_X['2019-03'])\nprint(nes_metric(validate_y['2019-03'].values, train_y_hat))\nprint('04')\n\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2014-04'])\nprint(nes_metric(train_y['2014-04'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2015-04'])\nprint(nes_metric(train_y['2015-04'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2016-04'])\nprint(nes_metric(train_y['2016-04'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,validate_X['2017-04'])\nprint(nes_metric(validate_y['2017-04'].values, train_y_hat))\n\n\n\nprint('05')\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2014-05'])\nprint(nes_metric(train_y['2014-05'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2015-05'])\nprint(nes_metric(train_y['2015-05'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2016-05'])\nprint(nes_metric(train_y['2016-05'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,validate_X['2017-05'])\nprint(nes_metric(validate_y['2017-05'].values, train_y_hat))\n#train_y_hat = recursive_forecast(lgb_model,validate_X['2019-05'])\n#print(nes_metric(validate_y['2019-05'].values, train_y_hat))\nprint('06')\n\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2014-06'])\nprint(nes_metric(train_y['2014-06'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2015-06'])\nprint(nes_metric(train_y['2015-06'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2016-06'])\nprint(nes_metric(train_y['2016-06'].values, train_y_hat))\n#train_y_hat = recursive_forecast(lgb_model,validate_X['2017-06'])\n#print(nes_metric(validate_y['2017-06'].values, train_y_hat))   # [DatetimeIndex(['2017-06-05 20:00:00'], dtype='datetime64[ns]', freq=None)] are in the [index]\"\n\n\nprint('07')\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2014-07'])\nprint(nes_metric(train_y['2014-07'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2015-07'])\nprint(nes_metric(train_y['2015-07'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2016-07'])\nprint(nes_metric(train_y['2016-07'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,validate_X['2017-07'])\nprint(nes_metric(validate_y['2017-07'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,validate_X['2019-07'])\nprint(nes_metric(validate_y['2019-07'].values, train_y_hat))\nprint('08')\n\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2014-08'])\nprint(nes_metric(train_y['2014-08'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2015-08'])\nprint(nes_metric(train_y['2015-08'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2016-08'])\nprint(nes_metric(train_y['2016-08'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,validate_X['2017-08'])\nprint(nes_metric(validate_y['2017-08'].values, train_y_hat))\n\n\nprint('10')\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2014-10'])\nprint(nes_metric(train_y['2014-10'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2015-10'])\nprint(nes_metric(train_y['2015-10'].values, train_y_hat))\n#train_y_hat = recursive_forecast(lgb_model,train_X['2016-10']) #[DatetimeIndex(['2016-10-05 02:00:00'], dtype='datetime64[ns]', freq=None)] are in the [index]\"\n#print(nes_metric(train_y['2016-10'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,validate_X['2017-10'])\nprint(nes_metric(validate_y['2017-10'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,validate_X['2019-10'])\nprint(nes_metric(validate_y['2019-10'].values, train_y_hat))\n\nprint('11')\n\n\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2014-11'])\nprint(nes_metric(train_y['2014-11'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2015-11'])\nprint(nes_metric(train_y['2015-11'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,train_X['2016-11'])\nprint(nes_metric(train_y['2016-11'].values, train_y_hat))\ntrain_y_hat = recursive_forecast(lgb_model,validate_X['2017-11'])\nprint(nes_metric(validate_y['2017-11'].values, train_y_hat))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05e39379-1e5b-4682-93be-2b725fb8d716","_cell_guid":"1dfb7ed0-32d3-4e39-95a7-689140639291","trusted":true},"cell_type":"code","source":"lgb.plot_importance(lgb_model)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce87a9ea-e693-4847-914b-dc59ce034432","_cell_guid":"5e1978f3-3e0d-4c10-a83c-d54d4cd8c0da","trusted":true},"cell_type":"code","source":"test_y_hat_1 = recursive_forecast(lgb_model,test_X_1)\ntest_y_hat_2 = recursive_forecast(lgb_model,test_X_2)\ntest_y_hat_3 = recursive_forecast(lgb_model,test_X_3)\ntest_y_hat_4 = recursive_forecast(lgb_model,test_X_4)\ntest_y_hat_5 = recursive_forecast(lgb_model,test_X_5)\ntest_y_hat_6 = recursive_forecast(lgb_model,test_X_6)\ntest_y_hat_7 = recursive_forecast(lgb_model,test_X_7)\ntest_y_hat_8 = recursive_forecast(lgb_model,test_X_8)\n\nplt.plot(test_y_hat_1)\nplt.plot(test_y_hat_2)\nplt.plot(test_y_hat_3)\nplt.plot(test_y_hat_4)\nplt.plot(test_y_hat_5)\nplt.plot(test_y_hat_6)\nplt.plot(test_y_hat_7)\nplt.plot(test_y_hat_8)\n# submit\nSubmitRaw = data_file + '/submission.csv'\nsubmit_empty = pd.read_csv(SubmitRaw, index_col=0)\n\ntest_all_submission = np.vstack((test_y_hat_1, test_y_hat_2, test_y_hat_3))\nsubmit_df = pd.DataFrame(test_all_submission, columns=submit_empty.columns)\nsubmit_df.to_csv(\"./submission_lgb_2.csv\")\n\n\ntest_all_submission_final = np.vstack((test_y_hat_4, test_y_hat_5, test_y_hat_6, test_y_hat_7, test_y_hat_8))\nsubmit_df_final = pd.DataFrame(test_all_submission_final, columns=submit_empty.columns)\nsubmit_df_final.to_csv(\"./submission_lgb_2_final.csv\")\n\n# add lgb predict to lgb_df\n\n\nlgb_df.loc[test_X_1.index, 'lgb_yhat'] = test_y_hat_1\nlgb_df.loc[test_X_2.index, 'lgb_yhat'] = test_y_hat_2\nlgb_df.loc[test_X_3.index, 'lgb_yhat'] = test_y_hat_3\nlgb_df.loc[test_X_4.index, 'lgb_yhat'] = test_y_hat_4\nlgb_df.loc[test_X_5.index, 'lgb_yhat'] = test_y_hat_5\nlgb_df.loc[test_X_6.index, 'lgb_yhat'] = test_y_hat_6\nlgb_df.loc[test_X_7.index, 'lgb_yhat'] = test_y_hat_7\nlgb_df.loc[test_X_8.index, 'lgb_yhat'] = test_y_hat_8\n\nprint(lgb_df.loc['2018-02-01':'2018-02-07', 'lgb_yhat'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"489ee272-4a5b-4c45-ab74-5008c1fc1b08","_cell_guid":"0973ad58-89b6-423d-aff9-a79cd6d3e62c","trusted":true},"cell_type":"code","source":"lgb_df.loc['2019-02-01':'2019-02-07', 'lgb_yhat'].plot()\nlgb_df.loc['2019-04-01':'2019-04-07', 'lgb_yhat'].plot()\nlgb_df.loc['2019-06-01':'2019-06-07', 'lgb_yhat'].plot()\n\nlgb_df.loc['2019-08-01':'2019-08-07', 'lgb_yhat'].plot()\nlgb_df.loc['2019-11-01':'2019-11-07', 'lgb_yhat'].plot()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3568a72b-f74e-4b36-a12e-3843af56c10c","_cell_guid":"5393a778-95a0-4715-940c-100186c68660","trusted":true},"cell_type":"code","source":"dnn_df = lgb_df.copy()\ndnn_df.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f01ccd7-05ae-4d73-8195-d8a0cec46126","_cell_guid":"4492f9e4-4875-4162-a993-8cdccc4e927f","trusted":true},"cell_type":"code","source":"# replace with standard DNN\nfrom tensorflow.keras import regularizers\nimport keras.backend as K\nfrom sklearn.utils import shuffle\nfrom keras.layers import Dense, LSTM, Dropout\nfrom keras.models import Sequential\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# split_train/test\ntrain_df = dnn_df['2013-01':'2016-12'].copy()\nvalidate_df = dnn_df['2017-01':'2019-10'].copy()\ntest_df_1 = dnn_df['2018-02-01':'2018-02-07']\ntest_df_2 = dnn_df['2018-08-01':'2018-08-07']\ntest_df_3 = dnn_df['2018-11-01':'2018-11-07']\ntest_df_4 = dnn_df['2019-02-01':'2019-02-07']\ntest_df_5 = dnn_df['2019-04-01':'2019-04-07']\ntest_df_6 = dnn_df['2019-06-01':'2019-06-07']\ntest_df_7 = dnn_df['2019-08-01':'2019-08-07']\ntest_df_8 = dnn_df['2019-11-01':'2019-11-07']\n\n\ndef nes_loss_1D(y_ture, y_pred):\n    nse_ = 1 - K.sum((y_ture - y_pred) ** 2) / K.sum((y_ture - K.mean(y_ture)) ** 2)\n    loss = (1 - nse_) * 100\n    return loss\n\n\n# select features to build X y\nuse_cols = ['D1', 'D2', 'D3', 'D4', 'D5', 'rain_pred', 'D1_x_lag',\n            'D2_x_lag', 'D3_x_lag', 'D4_x_lag', 'D5_x_lag', 'rain_pred_x_lag', 'on_season', 'off_season', 'D1_on',\n            'D1_off', 'D2_on', 'D2_off',\n            'D3_on', 'D3_off', 'D4_on', 'D4_off', 'D5_on', 'D5_off', 'rain_pred_on',\n            'rain_pred_off', 'yhat', 'hour', 'days_in_month',\n            'dayofyear', 'weekofyear', 'month', 'quarter', 'year', 'D1_x_lag_std', 'D2_x_lag_std', 'D3_x_lag_std',\n            'D4_x_lag_std',\n            'D5_x_lag_std', 'future_5D_rain_sum_x_lag_std', 'rain_pred_x_lag_std',\n            'lgb_yhat']\n\n#use_cols = [ 'yhat','lgb_yhat','hour', 'days_in_month',            'dayofyear', 'weekofyear', 'month', 'quarter', 'year']\n\ntrain_no_nan = train_df[use_cols + ['Qi']].dropna(axis=0)\nvalid_no_nan = validate_df[use_cols + ['Qi']].dropna(axis=0)\nsubmit_no_nan = pd.concat([train_no_nan, valid_no_nan], axis=0)\n\ntrain_X_raw = train_no_nan[use_cols].copy()\nvalid_X_raw = valid_no_nan[use_cols].copy()\nsubmit_X_raw = submit_no_nan[use_cols].copy()\ny_train_raw = train_no_nan[['Qi']].values\ny_valid_raw = valid_no_nan[['Qi']].values\ny_submit_raw = submit_no_nan[['Qi']].values\n\ntest_X_raw_1 = test_df_1[use_cols].copy()\ntest_X_raw_2 = test_df_2[use_cols].copy()\ntest_X_raw_3 = test_df_3[use_cols].copy()\ntest_X_raw_4 = test_df_4[use_cols].copy()\ntest_X_raw_5 = test_df_5[use_cols].copy()\ntest_X_raw_6 = test_df_6[use_cols].copy()\ntest_X_raw_7 = test_df_7[use_cols].copy()\ntest_X_raw_8 = test_df_8[use_cols].copy()\n# scale the data\nsc = MinMaxScaler(feature_range=(0, 1))\ntrain_X_scaled = sc.fit_transform(train_X_raw.values)\nvalid_X_scaled = sc.transform(valid_X_raw.values)\nsubmit_X_scaled = sc.transform(submit_X_raw.values)\n\ntest_X_scaled_1 = sc.transform(test_X_raw_1.values)\ntest_X_scaled_2 = sc.transform(test_X_raw_2.values)\ntest_X_scaled_3 = sc.transform(test_X_raw_3.values)\ntest_X_scaled_4 = sc.transform(test_X_raw_4.values)\ntest_X_scaled_5 = sc.transform(test_X_raw_5.values)\ntest_X_scaled_6 = sc.transform(test_X_raw_6.values)\ntest_X_scaled_7 = sc.transform(test_X_raw_7.values)\ntest_X_scaled_8 = sc.transform(test_X_raw_8.values)\n\ndnn_model = Sequential()\ndnn_model.add(\n    Dense(10, input_shape=(train_X_scaled.shape[1],), kernel_initializer='glorot_uniform',\n          kernel_regularizer=regularizers.l2(0.0)))\ndnn_model.add(Dropout(0.1))\ndnn_model.add(Dense(1))\ndnn_model.compile(optimizer='adam', loss=nes_loss_1D)\nprint(dnn_model.summary())\ntrain_X, train_y = shuffle(train_X_scaled, y_train_raw, random_state=42)\n#train_X, train_y = shuffle(submit_X_scaled, y_submit_raw)#, random_state=42)\n\n\n# fit dnn_model\nhistory = dnn_model.fit(\n    train_X,\n    train_y,\n    epochs=3000,\n    batch_size=3000,validation_split =0.1)\n\nfigure,ax = plt.subplots()\nax.plot(history.history['loss'],\n         'b',\n         label='Training loss')\nax.plot(history.history['val_loss'],\n         'r',\n         label='Validation loss')\nax.set_ylim(0,100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6b400cf-92a9-4552-830a-370f1d8fe2b7","_cell_guid":"4e1f05ff-97e1-4123-80f9-fc5ad7a14e98","trusted":true},"cell_type":"code","source":"def nes_score1(y_ture, y_pred):\n    nes_score = 1 - np.sum((y_ture - y_pred) ** 2) / np.sum((y_ture - np.mean(y_ture)) ** 2)\n    score = nes_score * 100\n    return score\n\n\ndnn_y = dnn_model.predict(submit_X_scaled)\ndnn_yhat_df = pd.DataFrame(dnn_y, columns=['dnn_yhat'], index=submit_no_nan.index)\nif 'dnn_yhat' in dnn_df.columns:\n    dnn_df['dnn_yhat'] = dnn_yhat_df['dnn_yhat']\nelse:\n    dnn_df = pd.concat([dnn_df, dnn_yhat_df], axis=1)\nr_s = dnn_df[['Qi', 'dnn_yhat']].copy().dropna(axis=0)\nr_s = r_s.loc[:, ~r_s.columns.duplicated()]\n\nplt.plot(r_s['dnn_yhat'].values[-56:])\nplt.plot(r_s['Qi'].values[-56:])\nprint(nes_score1(r_s['Qi'].values, r_s['dnn_yhat'].values))\nprint('01')\nprint(nes_score1(r_s.loc['2014-01', 'Qi'].values, r_s.loc['2014-01', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2015-01', 'Qi'].values, r_s.loc['2015-01', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2016-01', 'Qi'].values, r_s.loc['2016-01', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2017-01', 'Qi'].values, r_s.loc['2017-01', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2018-01', 'Qi'].values, r_s.loc['2018-01', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2019-01', 'Qi'].values, r_s.loc['2019-01', 'dnn_yhat'].values))\n\nprint('02')\nprint(nes_score1(r_s.loc['2014-02', 'Qi'].values, r_s.loc['2014-02', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2015-02', 'Qi'].values, r_s.loc['2015-02', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2016-02', 'Qi'].values, r_s.loc['2016-02', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2017-02', 'Qi'].values, r_s.loc['2017-02', 'dnn_yhat'].values))\n\nprint('03')\nprint(nes_score1(r_s.loc['2014-03', 'Qi'].values, r_s.loc['2014-03', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2015-03', 'Qi'].values, r_s.loc['2015-03', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2016-03', 'Qi'].values, r_s.loc['2016-03', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2017-03', 'Qi'].values, r_s.loc['2017-03', 'dnn_yhat'].values))\nprint('04')\nprint(nes_score1(r_s.loc['2014-04', 'Qi'].values, r_s.loc['2014-04', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2015-04', 'Qi'].values, r_s.loc['2015-04', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2016-04', 'Qi'].values, r_s.loc['2016-04', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2017-04', 'Qi'].values, r_s.loc['2017-04', 'dnn_yhat'].values))\n\n\nprint('05')\nprint(nes_score1(r_s.loc['2014-05', 'Qi'].values, r_s.loc['2014-05', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2015-05', 'Qi'].values, r_s.loc['2015-05', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2016-05', 'Qi'].values, r_s.loc['2016-05', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2017-05', 'Qi'].values, r_s.loc['2017-05', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2019-05', 'Qi'].values, r_s.loc['2019-05', 'dnn_yhat'].values))\nprint('06')\n\nprint(nes_score1(r_s.loc['2014-06', 'Qi'].values, r_s.loc['2014-06', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2015-06', 'Qi'].values, r_s.loc['2015-06', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2016-06', 'Qi'].values, r_s.loc['2016-06', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2017-06', 'Qi'].values, r_s.loc['2017-06', 'dnn_yhat'].values))\nprint('07')\nprint(nes_score1(r_s.loc['2014-07', 'Qi'].values, r_s.loc['2014-07', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2015-07', 'Qi'].values, r_s.loc['2015-07', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2016-07', 'Qi'].values, r_s.loc['2016-07', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2017-07', 'Qi'].values, r_s.loc['2017-07', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2018-07', 'Qi'].values, r_s.loc['2018-07', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2019-07', 'Qi'].values, r_s.loc['2019-07', 'dnn_yhat'].values))\n\nprint(nes_score1(r_s.loc['2014-08', 'Qi'].values, r_s.loc['2014-08', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2015-08', 'Qi'].values, r_s.loc['2015-08', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2016-08', 'Qi'].values, r_s.loc['2016-08', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2017-08', 'Qi'].values, r_s.loc['2017-08', 'dnn_yhat'].values))\n\nprint('10')\nprint(nes_score1(r_s.loc['2014-10', 'Qi'].values, r_s.loc['2014-10', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2015-10', 'Qi'].values, r_s.loc['2015-10', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2016-10', 'Qi'].values, r_s.loc['2016-10', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2017-10', 'Qi'].values, r_s.loc['2017-10', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2018-10', 'Qi'].values, r_s.loc['2018-10', 'dnn_yhat'].values))\nprint('11')\nprint(nes_score1(r_s.loc['2014-11', 'Qi'].values, r_s.loc['2014-11', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2015-11', 'Qi'].values, r_s.loc['2015-11', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2016-11', 'Qi'].values, r_s.loc['2016-11', 'dnn_yhat'].values))\nprint(nes_score1(r_s.loc['2017-11', 'Qi'].values, r_s.loc['2017-11', 'dnn_yhat'].values))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"369f0b02-c198-4b45-a3f3-e839a607c567","_cell_guid":"78df5815-be37-424b-ab88-da2d6c582eb3","trusted":true},"cell_type":"code","source":"#print(test_X_scaled_1)\n# test set predict\ntest_y_hat_1 = dnn_model.predict(test_X_scaled_1)\ntest_y_hat_2 = dnn_model.predict(test_X_scaled_2)\ntest_y_hat_3 = dnn_model.predict(test_X_scaled_3)\ntest_y_hat_4 = dnn_model.predict(test_X_scaled_4)\ntest_y_hat_5 = dnn_model.predict(test_X_scaled_5)\ntest_y_hat_6 = dnn_model.predict(test_X_scaled_6)\ntest_y_hat_7 = dnn_model.predict(test_X_scaled_7)\ntest_y_hat_8 = dnn_model.predict(test_X_scaled_8)\n\n# plt.plot(test_y_hat_1)\n# plt.plot(test_y_hat_2)\n#plt.plot(test_y_hat_3)\n#plt.plot(test_y_hat_4)\n\nplt.plot(test_y_hat_8)\n\n# plt.plot(dnn_df.loc['2018-02-01':'2018-02-07',['yhat','lgb_yhat']].values)\n# plt.plot(dnn_df.loc['2018-08-01':'2018-08-07',['yhat','lgb_yhat']].values)\n#plt.plot(dnn_df.loc['2018-11-01':'2018-11-07', ['yhat', 'lgb_yhat']].values)\n#plt.plot(dnn_df.loc['2019-02-01':'2019-02-07', ['yhat', 'lgb_yhat']].values)\n\nplt.plot(dnn_df.loc['2019-11-01':'2019-11-07', ['yhat', 'lgb_yhat']].values)\n\n# submit\nSubmitRaw = data_file + '/submission.csv'\nsubmit_empty = pd.read_csv(SubmitRaw, index_col=0)\n\ntest_all_submission = np.vstack((test_y_hat_1.T, test_y_hat_2.T, test_y_hat_3.T))\nsubmit_df = pd.DataFrame(test_all_submission, columns=submit_empty.columns)\nsubmit_df.to_csv(\"./submission_dnn2.csv\")\n\n\ntest_all_submission_final = np.vstack((test_y_hat_4.T, test_y_hat_5.T, test_y_hat_6.T, test_y_hat_7.T, test_y_hat_8.T))\nsubmit_df_final = pd.DataFrame(test_all_submission_final, columns=submit_empty.columns)\nsubmit_df_final.to_csv(\"./submission_dnn2_final.csv\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}